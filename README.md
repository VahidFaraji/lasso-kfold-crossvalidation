# ğŸ“ˆ LASSO Regression with K-Fold Cross Validation

This project demonstrates how to apply **K-Fold Cross Validation** to tune the regularization parameter **Î»** for the **LASSO** regression model. The goal is to select the Î» that minimizes validation error and avoids overfitting or underfitting.

---

## ğŸ” Problem Overview

We simulate a sparse linear regression problem and evaluate how different Î» values affect the model's performance. Using **K-Fold Cross Validation**, we systematically:

- Split the data into `K` folds
- Train and validate LASSO models for different Î»
- Select the Î» with lowest average RMSE

---

## ğŸ§ª Methods

- LASSO implementation via **Coordinate Descent**
- `K = 5` folds cross-validation
- RMSE evaluation across validation sets

---

## ğŸ“ Files

- `code/lasso_cv.py` â€“ Cross-validation driver
- `code/lasso_ccd.py` â€“ LASSO implementation using coordinate descent
- `results/rmse_vs_lambda.png` â€“ RMSE curve across Î» values
- `results/optimal_lambda_result.png` â€“ Prediction with best Î»

---

## ğŸ“Š Example Output

<p float="left">
  <img src="results/rmse_vs_lambda.png" width="400"/>
  <img src="results/optimal_lambda_result.png" width="400"/>
</p>

---

## ğŸ§  Author

**Vahid Faraji**  
Masterâ€™s Student â€“ Machine Learning, Systems and Control  
Lund University, Sweden
